---
title: "Customer Segmentation Analysis"
author: "Nicholas Chavez"
format: pdf
---
# June 6th, 2025
**Business Task:** Using Online Retail data donated to UC Irvine on 11/5/2015 that recorded all transactions occurring between 01/12/2010 to 19/12/2011 for a UK-based registered non-store online retailer, I am prompted with the task of differentiating customer types to drive targeted marketing campaigns. 

## Setting up packages and loading data
```{r loading packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(plotly)
setwd("C:/Users/Nicho/OneDrive/Desktop/Projects")
sales_data <- read.csv("Online_Retail.csv")
```

## Explore Data
```{r exploring data}
head(sales_data)
tibble(arrange(sales_data, Quantity))
```
Customer ID is a unique identifier that can be used to create a summary data for frequency, InvoiceDate can be used to determine how recent orders are, and the sum of the product of Quantity and Unit price per CustomerID can be used to determine monetary spending habits per customer. Also, Invoice data is in character format, using lubricate is necessary to covert the data to date format. Additionally, Quantity can be negative, based on description of data seems to be salvage/unsaleable merchandise, this will have to be removed in the cleaning process.

## Data Cleaning and mutation: 
In this cleaning, I dropped null values for CustomerIDs, removed negative values for Quantity, added the TotalPurchase column which is the sum of the products of Quantity and UnitPrice, and reformatted InvoiceDate from Character String to Date. 
```{r cleaning data}
cleaned_sales <- sales_data |> 
  drop_na(CustomerID) |> 
  filter(Quantity > 0) |> 
  mutate(TotalPurchase = Quantity*UnitPrice) |> 
  mutate(InvoiceDate = mdy_hm(InvoiceDate)) 
```

## Data for Recency, Frequency, and Monetary Spending
In this section, I calculated new fields Recency, Frequency, and Monetary to help determine customer type. The first step was creating a reference date to compare dates to determine recency before using n_distinct() and sum() to create frequency and monetary.
```{r generating new data}
reference_date <- max(cleaned_sales$InvoiceDate) + days(1)
RFM_data <- cleaned_sales |> 
  group_by(CustomerID) |>
  summarise(
    recency = as.numeric(reference_date - max(InvoiceDate), units = "days"),
    frequency = n_distinct(InvoiceNo),
    monetary = sum(TotalPurchase)
  )
```

## Prepare for K-means
Standardize the data for comparability between different fields to prepare for K-means. 
```{r standardizing data}
Standardize <- function(x){
  (x - min(x))/(max(x)-min(x))
} # Creating a function to standardize data

# Standardize all values for comparability
rfm_stz <- RFM_data |> 
  select(recency, frequency, monetary) |> 
  mutate(recency = Standardize(recency)) |> 
  mutate(frequency = Standardize(frequency)) |> 
  mutate(monetary = Standardize(monetary))
```
**check results**
```{r checking results}
summary(rfm_stz)
```
I need to determine which the numbers of clusters to specify. For this I will be using total-within cluster sum of squares (WCSS), creating a tibble of the results, and then plot the WCSS. I then determine which k to proceed with based on the Elbow Method.
```{r WCSS}
set.seed(123)
wcss <- map_dbl(1:10, ~{
  kmeans(rfm_stz[, c("recency", "frequency", "monetary")], centers = ., 
         nstart = 25)$tot.withinss
})
wcss_data <- tibble(
  k = 1:10,
  wcss = wcss
)

ggplot(wcss_data, aes(k, wcss)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(color = "red", size = 3) +
  labs(
    title = "Elbow Method for Optimal k",
    x = "Number of Clusters (k)",
    y = "Within-Cluster Sum of Squares (WCSS)"
  ) +
  scale_x_continuous(breaks = 1:10) +
  theme_minimal()
```
Based on the Graph, 4 is where the elbow appears and appears flat.

## K-Mean Clustering
Now I will run the K-means algorithim to group/segment the customers.
```{r kmean clusters}
set.seed(123)
km_out <- kmeans(rfm_stz, 4, nstart = 42)

km_out$centers
```
## Adding the clusters to the data set
```{r generating data}
clustered_sales <- rfm_stz |> 
  mutate(cluster = factor(km_out$cluster))
```
## Visualize the data
```{r}
#| eval: false
#| include: false
plot_ly(clustered_sales, x = ~recency, y = ~frequency, z = ~monetary,
        color = clustered_sales$cluster)
```
```{r defining clusters}
ggplot(clustered_sales, aes(x = monetary, y =frequency))+
  geom_point(aes(color = recency)) +
  facet_wrap(~cluster)
```
**Findings:** Through K-mean clustering I was able to determine 4 separate groups of customers. The characteristics of each group is as follows: 

* **Group 1:** Mostly recent with higher spread in spending and frequency.
* **Group 2:** Third most recent with lower levels of spending and frequency. Has a few higher spenders.
* **Group 3:** Least recent with lower levels of spending and frequency.
* **Group 4:** Second most recent with lower levels of spending and frequency. 

## Restructure Data to prepare for further visualization
Here I will be taking the RFM_data that contains customerIDs then include the newly determine clusters before renaming them for interpretability.
I will define each group based on their attributes. Group 1 will be label High Value, Group 2 would be Churning, Group 3 will be Inactive, and Group 4 are Occasional. All are defined based on the characteristics above.

```{r generating final dataset}
cleaned_clusters_sales <- RFM_data |> 
  mutate(Cluster = clustered_sales$cluster) |> 
  mutate(
    segment = case_when(
      Cluster == 1 ~ "High Value",
      Cluster == 2 ~ "Churning",
      Cluster == 3 ~ "Inactive",
      Cluster == 4 ~ "Occasional"
    )
  )
head(cleaned_clusters_sales)
```

## Verification
I was able to apply the clusters back into the dataset because the order was the same, however that method can be prone to error. So to verify I unstandardized the values to verify they were correct. This method of verification was able to work because the ID column was drop when standardizing the data, so reverting the calculation should give the data set with the clusters the same value as the dataset with the ID.
```{r ensuring data integrety}
unstandardize <- function(standardized_value, x){
  standardized_value*(max(x)-min(x)) + min(x)
}
check <- clustered_sales |> 
  mutate(
   recency = unstandardize(recency, RFM_data$recency),
   frequency = unstandardize(frequency, RFM_data$frequency),
   monetary = unstandardize(monetary, RFM_data$monetary)
  )
head(cleaned_clusters_sales)
head(check)
tail(cleaned_clusters_sales)
tail(check)
```
When looking at the table I looked to see if the recency,frequency, monetary, and cluster values matched. So this checks out and the data is associate with the correct customerID's. Since the data is now confirmed to be credible I can export the data to be used in other visualization tools.

```{r creating new data file}
write.csv(cleaned_clusters_sales, "Segmented_data_Irvine.csv", 
          row.names = FALSE)
```

